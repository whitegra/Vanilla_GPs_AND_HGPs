# -*- coding: utf-8 -*-
"""Heteroskedastic_GP_2D_TPS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AkwoC0qK3SIAEXIgRp0raPzaT-mOgcDW
"""

!pip install tensorflow
!pip install gpflow==2.9.1

import numpy as np
import gpflow
from scipy.stats import norm
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter, MaxNLocator

import numpy as np
import itertools

import gpflow
from gpflow.mean_functions import Constant
from gpflow.likelihoods import HeteroskedasticTFPConditional
import tensorflow as tf

import pandas as pd

# DEFINE HETEROSKEDASTIC GP MODEL + TRAINING
def train_noise_model(X, y):
    kernel = gpflow.kernels.SquaredExponential(lengthscales=0.3)
    noise_model = gpflow.models.GPR(data=(X, y), kernel=kernel, mean_function=None)
    opt = gpflow.optimizers.Scipy()
    opt.minimize(noise_model.training_loss, noise_model.trainable_variables)
    return noise_model

# Expected Improvement function for minimization
def expected_improvement(y_best, f_mean, f_var):
    variance = np.maximum(f_var, 1e-9)  # Ensure variance is non-negative
    std_dev = np.sqrt(variance)
    delta = y_best - f_mean
    with np.errstate(divide='ignore'):
        Z = delta / std_dev
        ei = delta * norm.cdf(Z) + std_dev * norm.pdf(Z)  # Correctly adjusted for minimization
        ei[std_dev == 0.0] = 0.0
    return ei

def train_heteroskedastic_gp_model(X, y, noise_variance):
    kernel = gpflow.kernels.SquaredExponential(lengthscales=0.3)
    heteroskedastic_gp_model = gpflow.models.GPR(data=(X, y), kernel=kernel, mean_function=None)

    # During predictions, incorporate the noise variance manually
    def custom_predict(X_new):
        mean, variance = heteroskedastic_gp_model.predict_f(X_new)

        # Ensure noise_variance is broadcasted correctly
        if noise_variance.shape[0] != X_new.shape[0]:
            expanded_noise_variance = tf.broadcast_to(noise_variance.mean(), variance.shape)
        else:
            expanded_noise_variance = tf.broadcast_to(noise_variance, variance.shape)

        # Add the expanded noise variance to the predicted variance
        variance = variance + expanded_noise_variance

        return mean, variance

    # Attach the custom predict method to the model
    heteroskedastic_gp_model.custom_predict = custom_predict

    # Train the model
    opt = gpflow.optimizers.Scipy()
    opt.minimize(
        heteroskedastic_gp_model.training_loss,
        heteroskedastic_gp_model.trainable_variables
    )

    return heteroskedastic_gp_model

# Load data from CSV
data = pd.read_csv('DOE2factor - DOE2factor_ALL.csv')
x1 = data['X1'].values.reshape(-1, 1)
x2 = data['X2'].values.reshape(-1, 1)
y1 = data['F1'].values.reshape(-1, 1)
y2 = data['F2'].values.reshape(-1, 1)

y1 = y1 * -1
y2 = y2 * -1

print(y1)
print(y2)

# Combine x1 and x2 into a single feature matrix X
X = np.column_stack((x1, x2))

# Min and Max for X1 and X2
min_x1 = 0.25
max_x1 = 8.5
min_x2 = 1
max_x2 = 14

# Define grid limits for X1 and X2
x1_grid = np.linspace(min_x1, max_x1, 100)
x2_grid = np.linspace(min_x2, max_x2, 100)
x1_grid, x2_grid = np.meshgrid(x1_grid, x2_grid)

# Generate grid for prediction (100x100 grid points)
X_plot = np.array([x1_grid.ravel(), x2_grid.ravel()]).T

# Train noise model for y1
noise_model_y1 = train_noise_model(X, y1)
pred_noise_var_y1, _ = noise_model_y1.predict_f(X_plot)
pred_noise_var_y1 = pred_noise_var_y1.numpy()

# Train heteroskedastic GP model for y1
heteroskedastic_gp_model_y1 = train_heteroskedastic_gp_model(X, y1, pred_noise_var_y1)

# Predict using the custom prediction method for y1
f_mean_y1, f_var_y1 = heteroskedastic_gp_model_y1.custom_predict(X_plot)

# Reshape as needed for plotting
f_mean_y1 = f_mean_y1.numpy().reshape(x1_grid.shape)
f_var_y1 = f_var_y1.numpy().reshape(x1_grid.shape)

# Train noise model for y2
noise_model_y2 = train_noise_model(X, y2)
pred_noise_var_y2, _ = noise_model_y2.predict_f(X_plot)
pred_noise_var_y2 = pred_noise_var_y2.numpy()

# Train heteroskedastic GP model for y2
heteroskedastic_gp_model_y2 = train_heteroskedastic_gp_model(X, y2, pred_noise_var_y2)

# Predict using the custom prediction method for y2
f_mean_y2, f_var_y2 = heteroskedastic_gp_model_y2.custom_predict(X_plot)

# Reshape as needed for plotting
f_mean_y2 = f_mean_y2.numpy().reshape(x1_grid.shape)
f_var_y2 = f_var_y2.numpy().reshape(x1_grid.shape)

# Calculate LCB and UCB for y1
LCB_y1 = f_mean_y1 - 1.96 * np.sqrt(f_var_y1)
UCB_y1 = f_mean_y1 + 1.96 * np.sqrt(f_var_y1)

# Calculate LCB and UCB for y2
LCB_y2 = f_mean_y2 - 1.96 * np.sqrt(f_var_y2)
UCB_y2 = f_mean_y2 + 1.96 * np.sqrt(f_var_y2)

# y_best = minimum value of y
y_best_f1 = np.min(y1)
y_best_f2 = np.min(y2)

EI_y1 = expected_improvement(y_best_f1, f_mean_y1, f_var_y1)
EI_y2 = expected_improvement(y_best_f2, f_mean_y2, f_var_y2)

# Function to plot heatmaps
def plot_heatmaps(x1_grid, x2_grid, f_mean_y1, LCB_y1, EI_y1, f_mean_y2, LCB_y2, EI_y2):
    # Ensure the arrays are reshaped correctly
    grid_shape = (x1_grid.shape[0], x2_grid.shape[1])
    f_mean_y1 = f_mean_y1.reshape(grid_shape)
    LCB_y1 = LCB_y1.reshape(grid_shape)
    EI_y1 = EI_y1.reshape(grid_shape)
    f_mean_y2 = f_mean_y2.reshape(grid_shape)
    LCB_y2 = LCB_y2.reshape(grid_shape)
    EI_y2 = EI_y2.reshape(grid_shape)

    fig, axes = plt.subplots(2, 3, figsize=(20, 10), dpi=800)

    # Plotting for y1
    cf1 = axes[0, 0].contourf(x1_grid, x2_grid, f_mean_y1, cmap='coolwarm')
    c1 = axes[0, 0].contour(x1_grid, x2_grid, f_mean_y1, colors='black')
    axes[0, 0].scatter(x1, x2, color='white', edgecolor='black')  # Scatter X1 and X2 points in white
    axes[0, 0].clabel(c1, inline=True, fontsize=15, colors='black')
    axes[0, 0].set_title("y1 Mean", fontsize=20)
    axes[0, 0].set_xlabel('X1', fontsize=20)
    axes[0, 0].set_ylabel('X2', fontsize=20)
    axes[0, 0].tick_params(axis='both', which='major', labelsize=20)

    cf2 = axes[0, 1].contourf(x1_grid, x2_grid, LCB_y1, cmap='coolwarm')
    c2 = axes[0, 1].contour(x1_grid, x2_grid, LCB_y1, colors='black')
    axes[0, 1].clabel(c2, inline=True, fontsize=15, colors='black')
    axes[0, 1].set_title("y1 Lower Confidence Bound", fontsize=20)
    axes[0, 1].set_xlabel('X1', fontsize=20)
    axes[0, 1].set_ylabel('X2', fontsize=20)
    axes[0, 1].tick_params(axis='both', which='major', labelsize=20)

    cf3 = axes[0, 2].contourf(x1_grid, x2_grid, EI_y1, cmap='coolwarm')
    c3 = axes[0, 2].contour(x1_grid, x2_grid, EI_y1, colors='black')
    axes[0, 2].clabel(c3, inline=True, fontsize=15, colors='black')
    axes[0, 2].set_title("y1 Expected Improvement", fontsize=20)
    axes[0, 2].set_xlabel('X1', fontsize=20)
    axes[0, 2].set_ylabel('X2', fontsize=20)
    axes[0, 2].tick_params(axis='both', which='major', labelsize=20)

    # Plotting for y2
    cf4 = axes[1, 0].contourf(x1_grid, x2_grid, f_mean_y2, cmap='coolwarm')
    c4 = axes[1, 0].contour(x1_grid, x2_grid, f_mean_y2, colors='black')
    axes[1, 0].scatter(x1, x2, color='white', edgecolor='black')  # Scatter X1 and X2 points in white
    axes[1, 0].clabel(c4, inline=True, fontsize=15, colors='black')
    axes[1, 0].set_title("y2 Mean", fontsize=20)
    axes[1, 0].set_xlabel('X1', fontsize=20)
    axes[1, 0].set_ylabel('X2', fontsize=20)
    axes[1, 0].tick_params(axis='both', which='major', labelsize=20)

    cf5 = axes[1, 1].contourf(x1_grid, x2_grid, LCB_y2, cmap='coolwarm')
    c5 = axes[1, 1].contour(x1_grid, x2_grid, LCB_y2, colors='black')
    axes[1, 1].clabel(c5, inline=True, fontsize=15, colors='black')
    axes[1, 1].set_title("y2 Lower Confidence Bound", fontsize=20)
    axes[1, 1].set_xlabel('X1', fontsize=20)
    axes[1, 1].set_ylabel('X2', fontsize=20)
    axes[1, 1].tick_params(axis='both', which='major', labelsize=20)

    cf6 = axes[1, 2].contourf(x1_grid, x2_grid, EI_y2, cmap='coolwarm')
    c6 = axes[1, 2].contour(x1_grid, x2_grid, EI_y2, colors='black')
    axes[1, 2].clabel(c6, inline=True, fontsize=15, colors='black')
    axes[1, 2].set_title("y2 Expected Improvement", fontsize=20)
    axes[1, 2].set_xlabel('X1', fontsize=20)
    axes[1, 2].set_ylabel('X2', fontsize=20)
    axes[1, 2].tick_params(axis='both', which='major', labelsize=20)

    plt.tight_layout()
    plt.show()

# Plot the heatmaps
plot_heatmaps(x1_grid, x2_grid, f_mean_y1, LCB_y1, EI_y1, f_mean_y2, LCB_y2, EI_y2)

# Flatten the grids and EI/LCB arrays
x1_flat = x1_grid.ravel()
x2_flat = x2_grid.ravel()

EI_y1_flat = EI_y1.ravel()
LCB_y1_flat = LCB_y1.ravel()

EI_y2_flat = EI_y2.ravel()
LCB_y2_flat = LCB_y2.ravel()

# Find the indices of the top EI values and minimum LCB values
top_ei_y1_idx = np.argmax(EI_y1_flat)
top_lcb_y1_idx = np.argmin(LCB_y1_flat)

top_ei_y2_idx = np.argmax(EI_y2_flat)
top_lcb_y2_idx = np.argmin(LCB_y2_flat)

# Get the corresponding x1, x2 values for these indices
top_ei_y1_point = (x1_flat[top_ei_y1_idx], x2_flat[top_ei_y1_idx], EI_y1_flat[top_ei_y1_idx])
top_lcb_y1_point = (x1_flat[top_lcb_y1_idx], x2_flat[top_lcb_y1_idx], LCB_y1_flat[top_lcb_y1_idx])

top_ei_y2_point = (x1_flat[top_ei_y2_idx], x2_flat[top_ei_y2_idx], EI_y2_flat[top_ei_y2_idx])
top_lcb_y2_point = (x1_flat[top_lcb_y2_idx], x2_flat[top_lcb_y2_idx], LCB_y2_flat[top_lcb_y2_idx])

# Print the results
print("Top EI point for y1 (x1, x2, EI):", top_ei_y1_point)
print("Top LCB point for y1 (x1, x2, LCB):", top_lcb_y1_point)

print("Top EI point for y2 (x1, x2, EI):", top_ei_y2_point)
print("Top LCB point for y2 (x1, x2, LCB):", top_lcb_y2_point)

# Plot variance heatmaps to see if there's any variation
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.contourf(x1_grid, x2_grid, f_var_y1, cmap='coolwarm')
plt.title("Variance for y1")

plt.subplot(1, 2, 2)
plt.contourf(x1_grid, x2_grid, f_var_y2, cmap='coolwarm')
plt.title("Variance for y2")

plt.show()